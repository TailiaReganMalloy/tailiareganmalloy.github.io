---
title: "Generative AI Governance and Ethics Conference CMU"
date: 2025-03-12
draft: false 
toc: false
layout: post
permalink: /posts/2025-03-12
author_profile: false
tags:
  - Conference
  - Artificial Intelligence
  - Social Science
---

These are the notes that I took during the Generative AI Governance and Ethics Conference CMU in March 2025. 

As with my past blog posts on conferences that have included my notes, the comments and descriptions are my own interpretation of the presentations given by the researchers below and may not necessarily reflect the thoughts or opinions of those researchers. I have tried to add some relevant links to recent research by these presenters to give more context to their work.

### Common phrases to search for:
- HIPPA - Medical applications of GAI.
- GAITAR - Educational applications of GAI.
- Agentic - Decision making with GAI.
- Policy - Discussions of public policy on GAI.
- Ethic - Discussions on ethical implications of GAI.

# Generative AI Ethics and Governance 

## Hoda Heidari 
### Framing and Introduction

Due process connection to AI governance? Issues of negative impacts of AI such as writing malware or inherent biases. If responsible AI is to foster innovation and growth while mitigating the issues with applications in AI. Governance is a system to control and direct and establishment towards its goals and it is held to account. Does not require the specific application of policy or slowing progress, but rather ensuring its sustained growth. 

- Academic governance involves standards and practice. 
- Government defines requirements and imposes sanctions. 
- Industry creates innovation and standards. 
- Civil society raises awareness and advocates (for whom?)

- My thoughts: Why is it the responsibility of civil society to raise awareness and advocate for the more vulnerable?

Different applications of AI have different requirements from a governance perspective, and this is exacerbated by the general purposes of modern AI methods that allow for wider application and user of AI technologies by people without special knowledge. 

## Theresa Mayer & DJ Patel
### Keynote and Fireside Chat 

An important idea is commitment to advocacy

- My Thoughts: What does it mean for CMU to be advocating, and for whom. 

Accountability, safety and security are critical to AI, while mitigating the costs associated with ensuring these aspects of AI governance are present. Federal level insights into AI governance and research in light of the new administration. DJ was the first chief data scientists of the united states. DJ mentioned that this is good to have as a discussion since that is an important aspect of AI governance. 

- My thoughts: It seems like a lot of the issues with AI are not that complicated, there are a lot of the biggest issues that are being ignored by governments and large companies. Why are we focusing on nuanced takes on the minutia of AI applications when these companies and governments are so blatant in their unethical use of AI? Companies flagrantly steal personal data and use AI technologies to get teenagers addicted to social media and the government refuses to do anything about it. 

Early work by the chief data scientists of the US were focused on the sharing of Data such as in healthcare contexts. (Related to HIPPA?). 

- If the early work of the US Chief Data Scientist was related to the personalization of the ownership of data then why are large companies that steal this data to train their models not held responsible by governments. 

There is still a lot of work left to be done in terms of sharing data in a way that can be useful for healthcare. Still need a lot in terms of infrastructure, etc. 

Q: What should the priorities of the Chief data scientists be? 

A: Much of the work of the chief data scientist is actually leveraging the public sector. The goals of this work should be done to make large economic impact, impact a large number of Americans, and improve the quality of life for the most disadvantage, and align with presidential priorities. 

- My thoughts: Should AI governance be aligned with presidential priorities? 

"Scout and Scale" methods of looking for "experiments" that are being done in the real world already, find the ones that work and then scale them to larger applications.   DJ claimed 'we don't need government to do a lot of the positive impacts that take place in AI governance'. 

-  My thoughts: Is this really outside of the scope of the government? Why is it the responsibility of small groups to prevent AI from destroying people's lives when that is the job of the government. 

During the height of AI doomerism, DJ and the white house were working on addressing some of the issues such as disinformation. 
- My thoughts: Has the goal of preventing misuse of AI in spreading misinformation largely been a failure? 

The plan for social media can't be to tell people to not post private or individual healthcare data on social media, because the people that use these resources have no other solution to their problems.

- My thoughts: Free universal healthcare  and universal basic income is a solution to their problem. 

What will be the impact of loosening regulations on the tech sector by the Trump Administration? Policy tends to evolve at a 10 year cycle, and very rarely do these policies get updated for new technologies after being passed. Compare this to large tech companies that produce one to three large major products per year, and large AI companies are producing new technologies at a 10x a year rate. 

- My thoughts: How much of the issue of the timescale of AI policy relevance is the slowness of policy creation compared to the speed of AI technologies compared to the issue of a failure and refusal to enforce these laws?

DJ also brings up the issue of enforcement, a major gap in the legislative side. 

- My thoughts: Was AI doomerism truly overblown in light of the widespread misuse of AI (stealing personal data and creating misinformation) and a failure and refusal of enforcing technology laws by governments.

DJ said that the amount of times that people in public policy around AI without using AI is a large amount. He also said 'if you are expecting good ideas to come from the government, good luck'. 

- My thoughts: This got a lot of laughs but it's not really funny and quite terrifying. This seems like more of an issue of prioritization and a willingness to hold large companies accountable. 

His solution is to leverage AI in the creation of ideas surrounding public policy. 

- My thoughts: I think this is a terrible idea.  

DJ also mentioned the necessity of having AI ethics and security classes in colleges that teach AI to undergrads. 

'Regulatory burden' is a big issue because it more significantly impacts smaller companies compared to larger ones. This is part of the reason for the large scale consolidation, because of the costs associated with conforming the company to regulations. 

DJ's advice to students: the most important thing is to learn how to lean and the non technical curriculum in your studies like economics, psychology, etc. The reason that linkedin didn't sell user data to other companies was because DJ and the CEO of linkedin took a philosophy class as an undergrad. 

- My thoughts: The 'learning to learn' is a cliche, while there is some truth to it in reality students won't keep paying large sums of money for degrees they don't think will result in a higher income in the future. 

DJ also says 'assume that your job and everything you know will be replaced and become obsolete'. Paranoia is good and the half-life of knowledge is getting smaller and smaller. Learning how to communicate in different contexts like law and economics or public policy, not just focusing on the language of technology. There is a major concern about how the country views and evaluates higher education, we previously have had the inherent idea that what we do is important and we need to get better and explaining our usefulness of our work. "We need to meet people where they are", engage the public in novel ways about why they should care about technologies. 

Question from the audience: With people like Musk becoming a part of the gov't, what do you foresee in terms of AI advancements? Are there areas you recognize that could benefit from such?

Answer: One way to think about it is to ask whether government efficiency is good, and most people would say yes and they are all trying to make work easier and more effective. A big challenge to this is a lack of appreciation for the jobs that governments do. Any approach of efficiency needs to be done with 'dignity' and a lack of 'cruelty'. People in industry viewed the Biden administration as too many breaks in AI advancement, and the Trump administration is all gas no breaks. It is more important to ask the question of what the issues that AI will solve are. More of nuance around where AI will be used (the best technologists can't answer this question)

- My thoughts: If the big technologists can't answer the question of what AI can do now and what it will be able to do in the future then why try to incorporate their thoughts into decision making now.

Question: What are your thoughts on AI as regards intellectual property rights, and what is the role of Government in regulating it? 

Answer: The definition of fair use is important and it is likely that the current government will loosen the definition of fair use, and that will likely lead to ethical issues. Society will enter a stage of 'AI fluency'

- My thoughts: How can we expect a future of AI fluency when our current society has no critical thinking skills and can't read past a 3rd grade level. 

Question: Please share your thoughts on the role of academia in enabling AI governance, research and policies?

Answer: Tighter connections to industry as opposed to government. Academia is setting the table for the future. 

- My thoughts: To me this ignores the idea of academic research as being foundational, not focused on specific applications, because if it comes into that then the focus of research will be profit and not the benefit of society. 

## Expert Panel 1: Ethics of Generative AI: New Issues and Challenges. 

### Moderator: Sina Fazelpour, 

### Atoosa Kasirzadeh, 

[A Taxonomy of Systemic Risks from General-Purpose AI](https://arxiv.org/pdf/2412.07780)

[Foundational Challenges in Assuring Alignment and Safety of Large Language Models](https://arxiv.org/pdf/2404.09932)

Risk governance for general purpose AI and lessons learned from industry and academia around the questions of AI risk governance. AIRG involves AI risk communication, identification and conceptualization risk management and mitigation and assessment and evaluation. Perspectives focusing on identification and conceptualization of general purpose AI. 

Usefulness of this area of study is to mitigate some of the risks of harm associated with GAI. Many of the instances of risk identification are concerned with charting the possible dangers and harms that GAI and LLMs in particular. One example of this is the stochastic parrots paper, and after this there was a large area of work that attempted to explain the space of all possible risks in a systematic and categorized manner. 

Highlighting areas of GAI risks that do not have tools associated with them are necessary and which tools can be applied onto existing issues in broad GAI models. 

The range of AI risk assessment extends from the lowest issues in the application of specific models onto individuals, and in the extreme case a systematic or extinction level risk in what AI will do. Some of these specific issues are discrimination, misinformation, malicious use, HCI user harm, information hazards and socioeconomic and environmental harms. 

- If we know what all of the risks are then we can see how they are being addressed and identify areas where risks exist without much work being done to address these issues. 

Different methods of identifying and managing risks are internal versus external audits of the risks associated with the applications of AI systems. 

- The current state of mitigating the harms associated with end user harm, at least from the perspective of the general public, in LLMs seems to be a disclaimer that the responses may be incorrect. What else can be done to communicate to end users the issues with the outputs of AI models. 

### David Danks, 

[Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation](https://dl.acm.org/doi/pdf/10.1145/3630106.3658946)

[Causal inference in cognitive neuroscience](https://wires.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/wcs.1650)

Harms and governance of generative AI. National AI Advisory Committee. Goal of the talk is to differentiate the issues related to GAI applications like LLMs that are distinct from some of the other possible harms in other contexts. One way to distinguish this is allocative harms (when you had an entitlement and did not get access to it) and representational harms (changes to representations in societies and individuals) that happen as a result of AI misuse.  An example of this is being incorrectly denied a loan for a car which has the representational harm of changing how an individual conceives of themselves, namely as someone who is not worth of getting a car loan. 

Representational harms are typically difficult to understand and see or measure in the real world compared to allocative harms. The way these can be raised in the real world are things like LLM outputs that bias specific races, genders, or nationalities (etc) as being better than other ones, which introduces a representational harm to people in the general public. This issue is typically not considered.

- Are there major differences between these issues from an engineering standpoint. Is it possible for a perfectly moral AI system from an allocative standpoint to have issues of representational harms. 

 Stochasticity adds issues to this in that it introduces a lack of trust. There are many cases of stochasticity that do not lead to more harm though, so we need to understand the connection between values and the acceptable technical constraints. Normal AI ethics assumes that 'we know when we have been harmed' but this isn't always the case like when our data is stolen. 

One solution to these issues is dynamic certification to build a formal model of socio-technical systems that allows us to express the constraints on models engaging in real world tasks that are necessary to prevent negative outcomes. 
 
### Francesca Rossi. 

AI ethics and governance in the era of agentic AI. Concretization of AI risks happens when it is occurring in the real world, such as when it is reported on by the news or generally known by the public. One example of this is the risks on education in plagiarism and the use of GAI in cheating. 

Agentic AI also have unique risks associated with it that are distinct from the issues associated with GAI research. The reason this is important is that there is a change in the context of the ways that these AI agents are beingused. Things like selecting the tools that are necessary to perform specific tasks or to make decisions to act in an environment based on a set of goals. This represents an AI openness that is different from past AI applications where the connections of the AI models were known, compared to now where the open-ended-ness means that there are tools available to the model that are not known. 

This has issues of opaqueness, complexity, and an usability to reverse harmful actions that are taken by AI agentic issues. The amplified risks associated with AI agents are things like fairness and value alignment that can expand the gap between performance and value alignment. In addition to amplified risks there are challenges that become harder such as compliance, transparency, etc. These issues exist in previous more standard uses of AI and GAI, but they are exacerbated in agentic AI. 

- My Thoughts: What is the upshot of this project of operationalizing AI ethics and governance? 

One is that this ethics is a company-wide approach to AI ethics. The decisions of the company must be made in relation to the AI ethics concerns, not just recommendations. Going beyond technical tools and regulation into actuall having processes of education, risk assessment and other important fprocesses. 

An integration of all governance dimensions, without this there is not going to be an adoption, meaning that they will just use their existing processes into the processes of risk assessment, etc, but with the aspects of AI application risks. Bringing together academia, the public, and other private companies in assessing these risks is important.

Question from moderator: what are the major achievements of AI ethics in the past 10 years and what are the remaining gaps? 

Answers: There has been a lot of research into AI trust, fairness, etc, to address the complexities off these terminologies. Journalists brought to attention some of the issues of the application of AI models 

- My thoughts: Like the assignment of higher probabilities of black prisoners to reoffend which lead to stricter sentencing and denial of parole. 

There are many multi-disciplinary approaches to understand AI ethics. The remaining major gapts are things like, if we have a multi-disiplinary and multi-stakeholder perspective then how do we decide what to do in the end, is talking enough? How can we ensure fairness across everyone involved in these processes. 

Answer David: There are many frameworks around the issues that there could be, which helps in identifying what the issues may be and how they can arise. But this also leads into some of the remaining issues in AI governance... There may be a lot of people that can give insight into the applications of fair GAI, but we need to significantly scale this because of the lack of transferability of the responsible AI thinking onto other companies and domains. So there needs to be practices that transcend specific. 

Answer from Francesca. There have been a lot of advancements since 2015 when the researchers were just first understanding what the issues are wand how they impact different industries or researchers. There was also a lack of efficiency and a high level of redundancy about these issues all over the world. Just a repeating of the same ideas in different contexts. Between coordination is important instead of just repeating the issues.  There are large differences in the maturity of the understanding of AI and the application of AI ethics across different regions of the world. We need to put AI ethics at the beginning of classes and think about them as being overarching in our understanding of the application of AI, not just at the end of courses as a afternote (if there is time) 

- My thoughts: I did this in my course. Maybe it would be better to begin with ethics in the future. 

Question from moderator: The issue of frameworks was discussed in all three presentations, and there is a sense in which GAI challenges these existing frameworks because GAI is inherently general purpose and it is difficult to understand what the end use case is. 

Answer from David: Pushback from the idea that GAI are 'general purpose' because things like LLMs are not fully GAI they are just more general than previous models. And a big issue with these models are the decision of a lower of a barrier of entry to using GAI, this is a good aspect but it also is associated with ethical issues, and there do not exist the tools to understand what the issues of the applications of these models are. The ease of dropping GAI models into many applications is a bigger concern than the general purposes of them. 

Answer from Francesca: IBM is a bit different in their use of AI models to solve business requirements, these LLMs may be built to solve a specific task and we know the people that will be using these technologies and we can train them to understand how to use these models in an ethical way. This allows models to be more specific to tasks and have less of an issue. There is also less of a systemic and existential risk associated in this way. IBM does not have a specific AI risk assessment process, but rather a specific risk assessment for each application. 

- My thoughts: Is that really a good idea?

Question from moderator: There was a discussion of all of the risks in each of the presentations, so this raises the question of a prioritization of the risks associated with AI. Different ideas like AI security or safety or trustworthiness have different priorities, how do we resolve these? 

Answer from Francesca: Companies should have processes of understanding what the specific risks associated with AI applications are, discussions with businesses and broad discussions to address this prioritization. Different cultures and governments also have different prioritizations, so it is important to be able to reflect these differences and not try to find a one size fits all approach to prioritization. Instead we need to be working within cultures, governments, communities, businesses, etc, in an open-source manner that can be tuned to these specific needs that are determined. 

- My thoughts:  I disagree with this from a standpoint of moral absolutism there should be general applicability of the priority of AI issues, and these should cross cultural boundaries. It is true that this is difficult but I would say that if these systems can't address this problem than they shouldn't be used. 

Answer: People interested in broad applications of AI systems need to be considerate of the broad risks, but users that are focused on more specific use cases can focus more on the issues and prioritization of risks, 

- My thoughts:  I also disagree with this for similar reasons as the last point, if people can't prioritize risks in the right way that is generally correct then they should be applying these technologies onto those applications. 

Answer from David: There is constant prioritizations of values during the development of any project, especially ones that use complex technologies. 

Question from the audience: Who is responsible for retraining employees and professionals who lose their jobs or whose position changes significantly due to AI?

Answer from Francesca: Education is continuous and people need to be maintaining their use of new technologies in their work. 

- My thoughts: Does this mean that part of the responsibility is on the individual?

 People need to be trained and have the skills to use AI in responsible ways 

- My thoughts: who is providing this training? Companies or governments? I think this answer was more so focusing on the idea of general applications and ways that GAI can impact the workforce, rather than the specific issue of what happens to people whose jobs become obsolete because of AI. To me the answer is universal basic income. 

Answer from David: Many programs and processes in society and governments exist to solve issues related to job loss and the changing of job categories. Historically, the ways that things like half a million people losing jobs in 10 years haven't impacted society in a way that we didn't know how to handle. But if this happens in the scale of 10 months or 10 weeks, then we won't know how to address this and we need to get ahead of the issues associated with large scale job displacement. 

Answer: Another solution to this question is the fact that the citizens need to be made aware that they need to retrain themselves, take online free courses and be proactive. 

- My thoughts: I mentioned this earlier but it bears repeating that one quarter of Americans do not read above the third grade level, and I believe that asking people to retrain themselves to become programmers is infeasible. 

Question from the audience: To Francesca: how do the climate considerations of AI fit into the agile governance framework? Especially the power intensity of genAI. Or does it not fit in?

Answer from Francesca: There are two dimensions of the issue of AI in sustainability, it goes in both directions. There are costs associated with training and even querying these models, but there are also potential benefits broadly, and also specific benefits to the environment. Usually the discussion of the environmental impacts do not distinguish between global and local impacts. Building large datacenters in towns will impact that area, but globally there is less impact and it is comparable to other technologies. 

Question from moderator: What is it that you think would be a good thing to stat doing now and that people would look back in 10 years and say, that was a good idea. 

Answer from David: The question we will be asking in 3 years is whether or not AI ethics goes the way of cybersecurity where it is very detached from the individual and highly specialized, or more so a concern of human-AI interaction, similar to the UI/UX design that tends to be more company specific. Any tech company has a UI/UX person but not all of them have a cybersecurity. He thinks that it will be the latter but he hops it will be the former. 

- My thoughts: I think this should be more similar to cybersecurity. We can't expect there to be teams of people at every company that understands the ethical and safety implications of their AI technologies, so compliance should be more similar to cybersecurity where external firms are hired to ensure safety. Having small teams in companies of 'AI engineers' would lead to a false sense of security that these technologies are being misused. 

## Lightning Talks

### Wesley Deng

Study on participatory genAI auditing. Building tools and processes with and for AI practitioners and end users. This was done with empirical studies and interviews with RAI practitioners and researchers. This resulted in the realization that most of the tools used by researchers are not applicable to real world applications. This required the development of tools and processes in the context of AU auditing, red teaming and impact assessment. One example of this is WeAudit, which involves the investigation and deliberation of perceived harms and biases of AI systems. This is done with both AI experts and non-AI experts. Part of this is to explore prompts that can lead to harmful outputs, report these based on their individual experience, and report them in a way that can be acted upon. Collective AI output auditing through discussions and the ability to learn how to use these systems. [We Audit](https://forum.weaudit.org/).

### Tzu-Sheng Kuo 

Empowering communities to collaboratively shape AI as common good. AI systems in use today are often not shaped in the context of improving the general social good in a manner similar to more tangible communal goods. Some of these systems are being used in community contexts to support existing community goods like schools, but this is typically done in a top down manner. This is an issue as it means that AI methods may be ignoring the needs of individual communities and their values. AI governance in policy craft, a method for communal discussions around policy development in a decentralized manner. One challenge of this from a bottom up perspective is the generality of policy. In case-grounded deliberation communities can discuss the validity of specific use cases, and introduce their own cases of appropriate use, and these proposals and critiques can be combined into the creation and refinement of policies. This was run in a class to design the policy around AI use.

### Mateo Dulce Rubio 

Using AI and data science to drive societal decision making and improve peace. This is a closed feedback loop that allows for continual advancement of the ideas of policy and the generation of new real world data, and refinement of policies. Today this is focused on post-conflict and peacebuilding scenarios. This is applied to responsible AI for humanitarian demining, this impacts the world with 70 countries and particularly the global south. Limitations in priorities and... RELand system can work to enhance the use of data and reports of landmines to create a pipeline of updating the data on landmine contamination. Systems that can do this with low and poor data can Deployed into Colombia and Afghanistan. 

### lindsay Fraff

Transportation policy. Fair optimization of utility subsidies. Universal basic mobility functions as a SNAP benefit of allowing people to subsidize their travel. E.g being given $100 to spend on busses, bikes, etc. One issue of this is that some communities have worse public transit and they require more money to access the same resources. This is done to maximize system-level job accessibility. This looks like giving a bus pass to every individual and then additional money to each individual that they can use on things like Ubers, etc. Looked at the area of Shadyside etc. Typically the number of jobs that are accessible by transit within 30 minutes is massively different in Pittsburgh, like hazelwood compared to Oakland. Through an optimal allocation there is a levelling out of the accessibility of the jobs. 

### Veronica Murige

Satellites and internet access. Many areas of the world have poor access to the internet and this impacts the ability to disseminate a high priority message such as environmental issues among others. There is a high cost associated with providing satellite internet access. This resulted in Camousat, a method for camouflaged satellite based internet access. Using the gaps in between access of these satellites to send messages that are related to public disasters. 


## Panel 2: Governmental Policies on Generative AI. 

### Marc Rotenberg

Marc wants to challenge a widely held assumption that AI research is way too far ahead of the law and policy for them to be effective with respect to it. There are hundreds of AI frameworks across different countries all with different methods of control. AI in democratic values is a white paper that explains a large collection of the different policies of these countries, and we can rank them in their pro-democracy. Most countries have policies that they say are meant to promote innovation and allow for safe secure and trustworthy AI. These encompass key goals for AI policy. 

["It is the policy of the United States to sustain and enhance Americaâ€™s global AI dominance in order to promote human flourishing, economic competitiveness, and national security."](https://www.whitehouse.gov/presidential-actions/2025/01/removing-barriers-to-american-leadership-in-artificial-intelligence/)

[There is a set of obligations that AI systems have](https://www.caidp.org/universal-guidelines-for-ai/).  

### Max Katz

Congressional perspective on AI, has been focusing on it since 2018. National security commission on AI was formed to tell the government how to manage AI. in 2020 the us passed the national AI legislation act, which remains the main policy on AI research and funding. The NSF funds 750 million a year in AI research. The AI risk management framework that was developed by NIST. National AI Research (NAIR). Many computational resources are available from the DoE and other areas, these can be applied for by academic researchers. The AI community is broad and not just AI Researchers, so thinking about computation as a common resource is important for the future. More recently there has been a focus on the legislation on AI regulation. One example is the sector specific case, these things often get passed in omnibus bills like the national defense legislation that gets passed every year to fund the army, and there are many instances of AI. Like making it illegal to produce revenge porn, which was passed in this type of omnibus bill. However this did not make it illegal to produce deepfakes of people even if it was created to do the same thing effectively. 

Other applications of policy are more broad in their applications and there are some pieces of legislation that hope to handle the brad applications of these technologies. What is the future of science if there is a strong integration of GAI, having humans be the pilots of these systems in their development and guidance of their experiments. This is a useful application that has as of yet been applied to limited use cases but could be applied onto science more broadly. The public description of the need of GAI is to solve all of the scientific issues that we might have, things like curing cancer and eliminating chronic disease. But if this is the case then why aren't we able to focus on these things directly instead of making products like ChatGPT. Much of the issues of the current applications is the fact that much of the scale of these technologies is in private companies. This means we need a public resource of AI systems that can use American data and technologies that surpass the abilities of systems built on publicly available data. 

### Hesse Dunietz

What are the other instruments that policy has besides just making laws and setting rules. Some of this is done by NIST (What is measurement technology?). NIST is interested in formalizations of standards rather than policy guidelines, which are different. AI Risk Mangement Framework is meant to make a detailed framework that can give actionable recommendations to individuals and groups that are interested in the use of AI systems. This requires a flexibility in the recommendations are that can account for the differences in a broad application space. 

- My thoughts: Are the other guidelines really not meant to be understandable by people who are actually making decisions about the application of these technologies? 

One of the methods of governing is the creation of frameworks, and another that is used by NIST is the development of standards. This is the establishment by consensus of rules that can be applied onto different contexts. 

- My thoughts: I don't really see how this is different from a guideline. A standard vs. a guideline. 

NIST is interested in the process of standardization and encouraging their use in the real world. 

Question from the moderator: How do you differentiate the needs of legislation for individual use cases compared to broad applications. 

Answer Marc: In Europe this is done in levels of potential harm that is possible, with the most constrained being applications that are illegal. In the US the legislation is typically done in the individual use case scenario like the use of deepfakes. The non-sector specific use case of Europe, e.g GDPR, may be more resilient. 

Answer Max: One of the big challenges isn't to think about how they should not be developed compared to instructions and guidance on how it should be created. Something like a Manhattan project style research objective to build artificial general intelligence, this can be more effective than the decentralized approach that is currently being done across different areas of academic and public resource. 

Answer from Hesse: Can't fully answer this question from the perspective of governance. Often there is a belief that the development of new technologies like GPT mean that we need to totally throw out the book on standardization, but this isn't the case and there are many applications of previous standards onto recent generative AI. In addition to this we can specialize existing approaches. 

Question from the audience: What is the definition of AGI to which the speakers prescribe. 

Answer Marc: Doesn't like the question because it isn't well formed. We used to talk about the moment of machine intelligence surpassing human intelligence with the worry being that we would not be able to govern or control AI once this happens. But this isn't necessarily the case. 

Question: There is an idea that there needs to be a balance between AI regulation and the advancement of these technologies, are there areas where there is bipartisan support for AI governance.

Answer: Yes, one is the application of laws that for instance exist to protect patients in healthcare settings that can be applied onto things like updating AI technologies in a way that ensures their use in the future. Another area is the cybersecurity concerns that are introduced by generative AI models and that there can be national security concerns from things like poisoning attacks. One example is the protection of people against generative AI use cases in deepfake and the stealing of personal information or the recreation of artistic property. One big challenge to this is respecting the first amendment. 

Question: What are the downstream effects of European regulations of AI? 

Answer: It will be positive, one example is an impact assessment of products that exist before they are created and a reassessment of these impact assessments. This is a common approach in other settings but haven't been applied onto AI research applications. Needs to be independent agencies to oversee authorities, public reporting requirements, and other openness, these things are required due to the high degree of advancement in these AI Areas. One of the unique ideas of the EU framework is the creation of 8 types of AI categories that cannot be created. This seems radical but in every other industry there are requirements of safety that need to be followed. 

Question: How will the current administration's policy differ from the previous and the EU. 

Answer: There was work being done with the Trump administration on the development of trustworthy AI. Hopefully there is a fair amount of continuity, and a continuation of the work being done in individual models. There may be changes in the use of language like DEI, but outside of those things. 

Question form the audience: How will the government remain objective with respect to the concerns of AI systems that are regulated. 

Answer: This is a big concern, from a policy perspective there may be some continuity. However there are going to be changes in the functioning of the government over sign, particularly inspectors general. Government efficiency may be important and there are sections within the government agencies that are responsible for finding fraud, but these are the people that were fired by the trump administration. 

Question: Other countries use AI regulatory sandboxes, should the US? 

Answer: Yes. 

- My thoughts: What is a regulatory sandbox? A regulatory sandbox is **a tool for developing evidence about how a new product, technology, or business model (innovation) works and the outcomes it produces**. Sandboxes operate under a special exemption, allowance, or other limited time-bound exception

Answer: There are challenges from a regulation standpoint introduced by the stochastic nature of modern AI technologies, and a lack of the Explainability of these models. For example if a model does not directly observe a users race but can infer it then it is possible that that is being used to make a determination. High public support for regulation of AI and the longer it takes for the federal government to take action means that individual states will like Colorado, Virginia and now Texas is adopting a law that appears similar to the EUAI act. 

Question from the audience: How can we stay updated on AI regulations and policies. 

Answer from the moderator: Call K&L Gates. 

Answer: Weekly news letters, asking GPT. 

Question: What is the importance of AI use in military settings. 

Answer: There is a lot of concern about how these uses should be developed because of the potential threats introduced by other countries developing these technologies. Also generals tend to want to retain the decision making ability, and do not want to give it over to an AI system. 


### (Re)working AI Sarah Fox: Tech solidarity lab

Worker's perspectives in AI deployment and development. One example of this is wellbeing for hostel housekeepers. Work has investigated how algorithmic management has impacted their work. Algorithmic management tools coordinate room attendant and supervisors. Some of the issues that these housemakers have with these algorithms is that it may change how their clean individual rooms, resulting in additional walking to and from rooms. 

One way to address these issues is a self sequencing feature that allows cleaners to determine what to do next and allow the workers to express their own expertise. Other companies are applying generative A methods for assigning schedules. 

Second project is in co-designing the future of transit work. Transit workers should have a seat at the table in discussions about the introduction of technologies into their work. One of the impacts of the increasing use of technology in transit operations is that these operators will be left with making decisions in the most difficult situations like when there is unpredictable roadway behavior including off road situations like assaults or the needs of the disabled. 

'Shock absorbers for Imperfect Automation'. The harms of not accounting for these shocks are the potential for punctuated points of intense management and less freedom of privacy. Collaborative human and machine teaming is one way to account for these issues, taking inspiration from the aviation industry. Goals of the work seek to improve the performance of the human-machine teams while taking into account the needs of the workers. 

### Marsha Lovett: 

Studying the impacts of generative AI in education, governing as well. Applications of generative AI is ultimately an open and empirical question without an automatic positive or negative. To promote effective education we need to understand them. Methods of measuring the impacts of CMU include GAITAR (which I applied for and did not get). This project collects data on student outcomes when the professors introduce AI into the classroom. This is done to eventually aid stanchers in the use of generative AI in the classroom. 

Some of the examples of challenges when using AI are negative impacts on students for using A as a substitute for learning., undermining incentives to perform the goals of the task. Challenges for educators include the academic integrity arms race or a changing of what the course design is. AI should ideally support learning as opposed to substituting it. 

- My thoughts: One of my biggest concerns of using AI is that it will benefit some students to the detriment of others. For example neurotypical students. 

The benefits of using AI should focus on cognitive offloading of extraneous tasks that are not related to the main course objectives but are required for the completion of tasks. 

Ongoing research at GAITER includes comparison of student writing before and after receiving feedback from A tool as compared to peers or instructors, which is being run across deferent studies. 

### Shiv Rao: Abridge

Applications of GenAI are moving quickly in healthcare. Abridge focuses on preventing clinicians burnout. Billions of dollars of costs in turnovers, the majority of clinicians and nurses want to quit. This burnout is associated with a tight requirement on productivity and expectations of a high amount of clerical work. At abridge this is done in targeted ways where there is a lot more overhead of clerical work. 

How has the medical industry prioritized use cases for AI? This can be thought of as a 2d comparison of risks and the number of decisions that are being made. Often times This company tends to focus more on the low risk areas.

Another way to think about the space of applications is to compare whether data improves the product and whether or not, assuming a a more perfect product is even better. 

- - My thoughts: I am not sure what that second point means.

Question from moderator: How much has gen AI changed the application that you work in. 

Shiv: Each of the areas of healthcare are thinking about how they can apply generative AI. The work at his company focuses on the side of the doctors and nurses. This often is back of the office work which is lower steaks, and allows for a better integration of human and AI models. On the payer side of Fen ai applications there is a lot of anger about how things get denied or approved 

- My thoughts: e.g. Luigi  

Educational disruptions in Gen AI include the idea that educational institutions are the controllers of knowledge, which is now changing, meaning that there is an existential crisis for many higher education workers.

Driving disruptions: There is a difficulty of what will happen once fully autonomous driving occurs, what will happen to all of these workers. The responsibility of retraining is unclear, who should be in charge of this retraining. In transportation there is a high variation of retraining, Waymo introduced a program with a CC in NY to retrain people that were made obsolete by self driving cars. This issue has come to a head and we need to focus on how to do retraining. 

Measuring the impact of Generative AI in different domains. 

In the educational setting it is important to collect rigorous data and keep the same goals of courses in mind while comparing how well those learning outcomes are achieved but it is difficult to measure actual learning with performance measures and their abilities rather than their perception of how much they have learned. Also focusing on the distribution of outcome changes to ensure that they are equitable, such as equitable access to tools . Other more qualitative measures like a sense of belonging and happiness in the courses. 

In the medical setting: Comparison to the previous applications of Gen AI as 'fast' thinking, but more agentic models with reasoning capabilities are more so related to 'slow' thinking and this is what is necessary for the future of these technologies. It is difficult to use standard measuring techniques to see how things are impacted in the healthcare sector. Some of them are more objective like the user experience metrics of burnout. Some of the less dangerous use cases, like having AI help write notes for patients, should be measured and governed differently than more dangerous applications. 

Issue with the lack of critical evaluation of applications of generative AI from the consumer side, the may not have the same level of subjective experience. It is important to think of the outcomes with and without the technologies. 

Question from the audience: Who has the responsibility of teaching end users about the role of AI. 

More than. higher education's responsibility to teach people about the workings of AI. Learning into critical thinking and being a constructive critic. 'learning to learn' skills. 

Question How does HIPPA compliance impact the development of the product? 

HIPAA protects everyone as patients to protect our own data in terms of where it goes and who an see it. Aggregation of fata involves things like removing personally identifiable information, there is also a direct to consumer application for the product. In this case because of the work that was necessary to control who sees data in the healthcare setting, the personal use side became more safe and protected for individuals than it needed to be. 

Question from the audience: How do we ensure that AI isn't introduced where it isn't needed. 

Labor advocacy groups, concerns about liability in the cases of accidents, because there is some precedent that they could be personally responsible if there is a crash, even if they are in a supervisory role. In some unions there is a veto power over the introduction of technologies and no replacement or displacement of workers. Other options are to work directly with congress to create highly stringent safety requirements for public safety. This allows for a top down and bottom approach to addressing the issues associated with the introduction of AI workers. 