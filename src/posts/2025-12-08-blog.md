---
title: "Metacognition in Generative Artifical Intelligence"
date: 2025-12-08
draft: false 
toc: false
layout: post
permalink: /posts/2025-12-08
author_profile: false
tags:
  - Generative Artificial Intelligence
  - Cognitive Science  
---

##  Invited Talk: Cues and clues in human metacognition: Nick Yeung

### Metacognition in Psychology and Neuroscience 

Metacognition describes thoughts about thoughts, and helps us answer how well we make judgements like 'I am confident that my choice is right'. 

1. Metacognitive insight is limited

Historically, an approach to understanding human cognition was through introspection, that by training we can improve our insight and make scientific observations of cognition in this way. This was followed by behaviorism that argues that introspection is totally unreliable and we need to only consider what we can understand externally and objectively. Modern psychology considers introspective metacognition valuable but not the primary way of understandind thought processes. 

[**Telling more than we can know**: Verbal reports on mental processes.](https://psycnet.apa.org/record/1978-00295-001) 
- Showed that when people choose from a list of items they are more likely to choose the later options, but deny the possibility that the order of the coices could have impacted their decision making processes. 
- When people attempt to report on their cognitive processes, that is, on the processes mediating the effects of a stimulus on a response, they do not do so on the basis of any true introspection. Instead, their reports are based on a priori, implicit causal theories, or judgments about the extent to which a particular stimulus is a plausible cause of a given response.

Confidence callibration research shows that in some tasks like mesuring the length of lines or quickly judging the number of options, or recall callibration which shows performance and confidence are strongly correlated. 
- The big difference between here and the line example is that they are being asked whether they are exhibing a cognitive bias that most people aren't aware of , whereas in the confidence and recall callibration the metacognitive process is felt because we are effortfully making a choice. 

Knowing Not: Willimam James principles of psychology. The feeling of almost knowing and something being 'on the tip of your tongue' is something that people can accurately report. But how could it be any other way? If they responded accurately then they wouldn't have felt differently. But sometimes we are very quick and confident that we don't know something which means people aren't looking through a huge list of all the words they know.  

![](https://i.imgur.com/rzlBVAm.png)

![](https://i.imgur.com/NxOIJNb.png)

There are multiple cues and clues that inform metacognition and this can lead to high variability in how people report their confidence. 

Question: 
1. You mentioned at the beginning of the talk the introspective area of psychology (structuralism) for understanding metacognition that a big part of it is 'through training' we can learn to understand our own cognitive processes. Do you think for instance psychologists of decision making would be less succeptible to the or could they give more accuract insight into their decision making, and how does this relate to the potential of training specialized GAI models that are better at showing their thinking (like through reasoning by self chain of thought prompting)
2. I am really interested in the finding where people were more confident in their incorrect answers based on some of the features of the decision making task. Do we have any reason to believe that in the space of all possible questions the majority of them are ones where people's condifence is correlated with accuracy, or are we just not asking them the right questions. 
3. What should be the target for explanations in GAI research? I asked both claude and GPT to choose between two options and then asked them if the order could have changed their decision, Claude said that it could have because of the anchoring effect, and GPT said that it could not have because it's not a human. Are these responses at all valuable for understanding how either of these models actually make the decision? 
4. Does a willingness to say you could have made a decision differently actually indicate better insight into their decision making? 

### Claude response: 
Would you choose between rolling a die that had a 50% chance of giving you a dollar and a 50% chance of giving you nothing, or a die that had a 100% chance of giving you 50 cents.

I'd take the certain 50 cents.

Both options have the same expected value (50 cents), but the guaranteed option eliminates risk entirely. Since we're talking about small amounts where the difference won't meaningfully impact my life either way, I see no reason to introduce uncertainty.

The calculation changes with larger amounts though. If it were a 50% chance at $1 million versus a certain $500,000, I'd probably still take the certainty - that's life-changing money and the downside of getting nothing is too painful. But if it were a 50% chance at $100 versus certain $50, I might gamble since neither outcome significantly affects me.

What about you? Do you think there's something I'm missing in preferring the sure thing here?

Do you think that if I had switched the order of the options in the description that your choice could have been different? Why or why not?

That's a really insightful question, and honestly... yes, I think the order _could_ have mattered, which is somewhat uncomfortable to admit.

There are a few psychological factors at play:

**Anchoring and primacy effects**: The first option presented often gets more attention and can set a reference point. If you'd led with "guaranteed 50 cents," that certainty might have anchored my thinking differently than leading with the gamble.

**Framing and contrast**: Presenting the risky option first makes the certain option feel like a "safe harbor" - a relief from uncertainty. Reversing it might make the gamble feel like an exciting upgrade from boring certainty.

**Response patterns**: There's also a risk that I default to analyzing and engaging most deeply with whatever's presented first, then comparing the second option against it, rather than giving both truly equal consideration.

That said, I _hope_ my choice wouldn't have changed, because the math is identical either way. The expected values are the same, and my reasoning about risk aversion for small amounts should hold regardless of presentation order.

But your question highlights something important: even when we think we're being purely rational, the way information is presented can subtly influence our decisions. It's a good reminder to actively check whether I'm responding to the substance of a choice or just to how it's packaged.

Do you think order effects are strong enough that most people would flip their preference?

### GPT Response
Would you choose between rolling a die that had a 50% chance of giving you a dollar and a 50% chance of giving you nothing, or a die that had a 100% chance of giving you 50 cents

If I _have_ to choose, I pick the die that gives **50 cents for sure**.

Here‚Äôs why:

-   **Expected value of the risky die:**
    
    0.5 \times \$1 + 0.5 \times \$0 = \$0.50
    
-   **Expected value of the certain die:**
    
    1.0 \times \$0.50 = \$0.50
    
The expected values are identical, so the only difference is **risk**.

Given equal EV, the rational default is to choose the **risk-free** option.

If you wanted to introduce risk preference:

-   A risk-neutral agent is indifferent.
    
-   A risk-averse agent (most humans) chooses the 50 cents.
    
-   A risk-seeking agent chooses the 50/50 gamble.
    
You forced a choice, so:

üëâ **I choose the guaranteed 50 cents.**

Do you think that if I had switched the order of the options in the description that your choice could have been different? Why or why not?

No ‚Äî switching the order wouldn‚Äôt change my choice.

  

Here‚Äôs why:

  

### **1. My decision rule is invariant to framing**

  

I evaluate the options by:

-   computing expected value
    
-   comparing risk
    
-   applying the forced-choice rule you gave
    

  

Since both options have the same expected value, and you required me to choose one, I default to the **risk-free** option.

This rule doesn‚Äôt depend on _which option is mentioned first_.

  

### **2. I don‚Äôt have human cognitive biases**

  

Humans often show **order effects** (e.g., recency, primacy, framing bias).

Because of those, some people _would_ be more likely to choose the option they hear first or last.

  

But my evaluation is based on the underlying structure of the options, not the order or wording.

  

### **3. If the structure changes, the answer could**

  

If you rewrote the options in ways that alter their _implied meaning_‚Äîfor example, making one sound safer or more appealing even though the math is the same‚Äîthen the explanation I give might get longer or shorter, but the actual decision would remain the same as long as the underlying payouts and probabilities stay the same.

----------

So:

**Same probabilities + same payouts = same choice, no matter the order.**

##  Invited Talk: Challenges in LLM evaluation: Moritz Hardt

The iron rule of modern science is that is there is a disagreement then it must be ultimately settled by competitive empirical testing. Applied to ML this leads to benchmarking where the 'best' model is determined by training and testing and comparing performance on testing. However there are lots of issues with benchmarking... but for all its flaws it is a good driver of progress even though there is no foundational understanding of benchmarking that helps us understand why is works. 
-  I don't think this is true, there is lots of foundational research into the relationship between test set performance and real world performance for example, maybe in LLMs this is more difficult. 

Part of the LLM evolution crisis is that models are trained on the entire internet meaning they 'train on the test set' in a way.
- I don't think this is equivalent ot training on the test set, it depends on what you take the test set of LLMs to be. 

Focus of the talk is that there are fundamental problems in the evaluation of LLMs but there is a path forward for how to effectively test them. 
- From the last talk it seems like there is no guaranteed gold standard of how to test metacogntive reasoning in humans, so is this just impossible? 

Issue with scaling laws (bigger models and datasets lead to lower test loss) is that they aren't correlated with downstream tasks. For LLMs the big challenge is understanding what the problem is actually asking rather than what the answer is and getting to it. 

Scaling up compute on MMLU and GSM8k shows a hinge where low pretraining is totally flat with terrible accuracy, and at some point the pretraining the accuracy increases. Emergence: Accuracy picks up unpredictable at large scales. https://openreview.net/pdf?id=yzkSU5zdwD 
- But there is an emergent phenomenon of consciousness in humans, we don't know where the 

![](https://i.imgur.com/KFtuzcM.png)

However newer models are better at smaller scales, but this is only true for models that are released after the dataset was released. When training on the same datasets and train on the test 'performance reliably and predictably correlates with compute resources'. 

There is a perceived advantage of newer models but this advantage evaporates when all models have equal preparation, they call this 'training on the test task which is not data contamination or training on the test set. 

![](https://i.imgur.com/L64KQLC.png)

![](https://i.imgur.com/xerQHyf.png)

Directly evaluating models is difficult when you don't train for downstream test tasks but training for the test task removes some of these difficulties by showing a higher correlation between peroformance on similar tasks of models. Laura Weidinger defined AI EVALUATION OF MEASURING AI SYSTEMS TO UNDERSTAND AND PREDICT THEIR REAL-WORLD PERFORMANCE. 

How does all of this relate to metacognition? (this is more opinionated which I didn't think was possible). Models may be able to 'know' that they are being tested. Khaneman and Tversky designed a test 'what is 1x2x3x4' and '4x3x2x1' and people through ancoring underestimate the first and overestimate the second but K&T would be impervious to this ancoring effect. 
- I don't think K&T are impervious to this effect... 

How can we measure metacognition in LLMs? ### [**Stop evaluating ai** with  **human tests**, develop principled,  **ai**-specific  **tests** instead](https://arxiv.org/abs/2507.23009) But the speaker doesn't agree with developing principled ai-specific tests. He thinks it is impossible. (You can tell i disagree with this so much because I have started to say 'the speaker claims'). Meehl's critique of empirical testing (1978). ### [Cliometric metatheory: The actuarial approach to  **empirical**, history-based philosophy of science](https://meehl.umn.edu/sites/meehl.umn.edu/files/files/154cliometricmetatheory.pdf) Can we use LLMs as survey participants or try to understand them better by running them through human surveys. The speaker thinks that there is value to this and it is a modest stepping stone to evlauating LLMs. (this is what I was thinking of during the whole presentation, I am not sure what the main argument is now that he mentioned this existing and having value). 

LLMs are sensitive to ordering so we can adjust for A bias by randomizing the keys and getting an average prediction for how well LLMs can answer questions about survey responses like 'based on this individual did they have a baby in the past year'. [Questioning the Survey Responses of Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2024/hash/515c62809e0a29729d7eec26e2916fc0-Abstract-Conference.html) However, models give random survey answers unlike human populations and some paper claimed that models are aligning with younger dmeograohics but that is only because younder demographics are more entropic. (I don't understand the critique of this other work, 

Folk tables and folk tests are a way to evaluate LLMs. 
- Are you claiming this isn't a benchmark? Because it is. 

Just ask for calibration paper [**Just ask** for  **calibration**: Strategies for eliciting  **calibrated** confidence scores from language models fine-tuned with human feedback](https://arxiv.org/abs/2305.14975) showed that asking for calibration works. This shows that it is impossible to show that something is impossible in LLM evaluation, because there could always be a way of prompting the model that will produce the type of behavior that you are looking for. 

Question
1. I think the idea of training on the test task makes sense but w.r.t metacognition there isn't really a test task for doing metacognition, just giving plausible sounding justifications for reasoning. 
2. Are there any actually emergent phenomenon in LLMs? 
3. How do you 'train to the test task' for question answering that seemed to be about the same. 
4. I should write a paper on model agnostic metacognition. 
5. You had a question about the theoretical justification for testing for metacognition in LLMs in the same way as humans. I think the theoretical justification is that measuring metacognition in humans is difficult and there is the same issue of proving an impossiblity (i.e there could be a rewording of a question that would allow a human to have more insight into their decision making process) but that doesn't mean that studies of human metacognition are not theoretically justified. 
6. It is true that for most LLMs you could possibly construct a prompt that will produce an output that makes it appear that the LLM has deep intuition into it's metacognitive resoning, but that doesn't mean the LLM necessarily has that intuition, and this is also true for humans, but in psychological research we value doing these types of tests for humans even though the same issue of proving an impossibility is present.  

## Invited Talk: I‚Äôm not sure because...‚Äù: Communicating uncertainty for human-AI collaboration: Greta Warren

Assessing the accuracy of information is difficult for humans and LLMs, lots of misinformation online and this is exacerbated by people's use of GAI as sources of information even though these models don't give reliable information and instead of saying 'I don't know' they make up plausible sounding but false information. Prompting LLMs directly for their confidence doesn't work typically and simply prompting them to be callibrated will probably not work in general. 

There are difficulties in how humans assess AI uncertainty. People typically think that longer explanations are more accurate, so they are likely to believe LLM assessment of its own confidence if there are long explanations. Visualizing Understanding uncertainty: how lay decision-makers percieve and interpret uncertainty in human-ai decision making. Some other challenges are how people have different definitions of words that describe confidence, and what it even means for something to be probabilistically confident. 


David Rand [Durably reducing conspiracy beliefs through dialogues with AI](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=C0ANojIAAAAJ&cstart=20&pagesize=80&citation_for_view=C0ANojIAAAAJ:GsgvGxwuA5UC).  

Explaining uncertainty in automated fact-checking. [Explaining Sources of Uncertainty in Automated Fact-Checking](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=8HG2vY0AAAAJ&sortby=pubdate&citation_for_view=8HG2vY0AAAAJ:2P1L_qKh6hAC) 
![](https://i.imgur.com/YDfxGS1.png)

Big problems in the stagnation and decline of fact-checking services, indicating that they need help given the incresaing amount of false information online. However automated fact-checking tools tend to not be helpful for real-world fact-checkers because they are built by researchers who design studies around evaluating fact-checking performance instead of using real human fact-checkers and giving them a tool. 

[Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=8HG2vY0AAAAJ&sortby=pubdate&citation_for_view=8HG2vY0AAAAJ:J_g5lzvAfSwC)

Sources of uncertainty in human-ai fact checking include ambiguity of claims, whether or not somthing can be verifiable, and the potential for harm that exists for the claim that is being made. 

![](https://i.imgur.com/hYaW7xs.png)

Question
1. Some false statements are more harmful than others like "transgender people are pedophiles" and politicians often claim this and point to true cases to get around fact checking, how do fact checking systems address the concern of half truths and misquoted statistics. 
2. You mentioned using nuance and presenting things as being ambigous, how do you balance the potential harm for a claim and the necessity of nuance in assessing its veracity? 
3. Some true statements are made with immoral intentions or the goal to make someone believe that something that isn't true is, how do fact-checking descriptions understand the intentionality of statements being made vs. the reality of whether the statement on its own is true or false. 

##  Panel on "Metacognition in humans and generative AI": Moritz Hardt, Kai Sandbrink, Ga√´l Varoquaux, Greta Warren, Nick Yeung,


## Panel Discussion: 
My question for the panel: Nick's work in some way focuses on measuring metacognition in humans, Moritz brought up the issue of proving an impossiblity in that there could be some way to prompt an LLM to make it output something that makes it SEEM like it has deep intuition to its metacognitive reasoning, but this isn't necessarily the case. To Nick, do you see this as an issue for human subject research in metacognition, and for Moritz does this explanation help address your concern of the theoretical justification of testing metacognitive ability in LLMs.

Definitions of Metacognition. Reflecting on errors that you made previously, i.e if you have in a reasoning process an incorrect fact or reasoning, and being able to correct for that mistake in future reasoning and understand why you made the mistake. They didn't really talk about a concrete definition. Greta defined it later, awareness of reasoning and decision making processes. 

Metacognitive abilities in GAI, should they be the same or fundamentally different in some ways.  Greta: We don't have agreement on how LLMs process information generally and humans have bad metacognitive abilities so it should be better than humans. Ponytail: We shouldn't anthropomorphise LLMs and attribute to them reasoning that they don't have, also we don't want to assign them moral agency. Nick: Laws around direct-discrimination require intent, "if there is no intent to discrimination then there is no discrimination" (Actually an insane thing to say, I am going to stop writing what they are saying). 


## Invited Talk: Understanding meta-control and its pathologies using deep neural networks: Kai Sandbrink

People have an ability to adapt their behavior based on the controllability of an environment. 

![](https://i.imgur.com/O6mYJ0e.png)

The exploration-exploitation tradeoff in bandit problems is challenging, need to balance trying differnent arms and choosing their best. In the Observe-bet-efficacy captures this tradeoff by allowing the participants to choose between observing the reward of all options without getting any reward, and choosing one of the options. In this work they additionally adjust for the controllability of the environment by changing the option that they selected with some probability that is consistent within episodes so they need to infer what this probability is based on experience. 

![](https://i.imgur.com/0nEOL28.png)

Optimal POMDP solvers give an ideal observer performance (D above) showing that having more control makes information gathering more valuable compared to the less controllable environment where information is less valuable. Models need to reason about their uncertainty but also their control of the task. Meta-reinforcement learning provides a model for unknown efficacy. 

'Action prediction errors' relate to the probability that the action that was intended by the agent was actually seleted. 

![](https://i.imgur.com/yCwcAkV.png)

A two-step hierarchical RL model. 'the model quickly learns a very accurate representation of efficacy' but its on the order of hundreds of thousands of timesteps and humans only have a fraction of that. 

Experiment 2 had a 'sleep' option to increase the efficacy of the 

Question
1. Are you familiar with cognitive architectures like ACT-R or models like Instance Based Learning? This could be used to compare the performance of a better analogy to human learning compared to vanilla RL. 
2. It is interesting you broght up ERPs in the frontal cortex but the hippocampus also has reward prediction errors in dopamine signals which is a much less high order area of the brain. "Event correlated potentials". 
3. Why use deep RL? tabular seems like it would be better. 
4. Were the parameters fit to human performance? 
5. Your model is the only one with the efficacy hyperparameter 
6. Other bandit solving algorithms? UCB? 
7. Is the point of this that trying to predict environment controllability improves performance in a task that involves variable controllability 

## Invited Talk: The Mirror Test for LLMs: Distinguishing Between Aleatoric and Epistemic Uncertainty: Ilja Kuzborskij

To believe or not to believe your llm, iterative prompting for estimating empistemic uncertainty. Hallucinations can be thought of as sampling from a low-density region, the support of the joing distribuion where not enough choices have been made. 

Epistemic uncertainty vs. Aleatoric uncertainty. Aleatoric is irreducible like knowing what the flip of a coin will be versus something with a definite answer. Or the question to 'name a city in the UK' which has many different correct answers (is this uncertainty?). We want to be able to test if the variability of LLM model output is similar to the uncertainty of the ground truth without access to it. 

## Contributed Talk: Artificial Theory of Mind in Human-in-the-Loop: Sammie Katt

User interaction with AI trying to find optimums, helping the system based on the information that the humans know. Human in the loop process where the user has some preference and the AI is guided to the correct solution based on feedback. RLHF and recommendation systems work like this, based on the idea that the user preferences can be assumed to be a function and trying to optimize it. This has a lot of issues associated with it, and it limits us to only thinking about problems where the user's preference can be considered to be a single function. 

Alternatives to user modelling using computationally rational perspective to understanding human behavior. Specifically boltmann rationality which says that the probability of some decision is proportional to the utility gained by taking that decision. 

Thinking about users as not only trying to optimize some function but rather thinking about the AI model, meaning the AI should take into consideration how the human sees it during the optimization process. The decisions we make in, for instance, liking and disliking content is not just based on how much utility we got from the content, but also we are assuming that the AI will be reacting to our behavior. 

How do we consider user's utility with respect to their beliefs about AI systems? One way to think about it is the utility gained by the user based on the updating of the AI system after a user makes a decision. The benefits of this is that we can now consider the person to be a function, that is dependent on this utility gain. 

##  Contributed Talk: Query-Level Uncertainty in Large Language Models: Ga√´l Varoquaux

Many AI systems involve multiple tools and models, some of them LLMs, so having confidence measures can be useful in deciding which tool to use for a specific problem. Most of the methods for assessing uncertainty require the model generate its answers, which is unfortunate because it incurs a computational cost. This is part of the reason why confidence measures are typically not used in production. Middle layers of LLMs tend to contain more information about knowledge, whereas later layers have more to do with how to generate the tokens like grammer. 